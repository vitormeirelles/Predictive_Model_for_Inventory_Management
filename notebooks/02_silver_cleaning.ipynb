{"cells":[{"cell_type":"code","source":["# Importar bibliotecas necessárias para Spark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, lit, when, trim, coalesce, create_map, array, approx_percentile\n","from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType, DecimalType\n","\n","# --- 1. Configuração de Lakehouses e Schemas ---\n","# Lakehouse da Camada Bronze (onde seus dados limpos estão)\n","bronze_lakehouse_name = \"Projeto_II_Bronze_\"\n","bronze_schema_name = \"Projeto_II_Bronze_\" # O schema onde suas tabelas Bronze foram salvas\n","\n","# Lakehouse da Camada Silver (destino dos dados transformados)\n","silver_lakehouse_name = \"Projeto_II_Silver_\"\n","silver_schema_name = \"Projeto_II_Silver_\" # O schema de destino dentro do Lakehouse Silver\n","\n","print(f\"Processamento da Camada Bronze -> Silver iniciado.\")\n","print(f\"Lendo do Lakehouse Bronze: '{bronze_lakehouse_name}', Schema: '{bronze_schema_name}'\")\n","print(f\"Escrevendo no Lakehouse Silver: '{silver_lakehouse_name}', Schema: '{silver_schema_name}'\")\n","print(\"-\" * 80)\n","\n","# --- 2. Processamento da Tabela 'cities_bronze' -> 'cities_silver' ---\n","print(\"\\n--- Processando cities_bronze -> cities_silver ---\")\n","try:\n","    df_cities_bronze = spark.read.format(\"delta\").table(f\"{bronze_lakehouse_name}.{bronze_schema_name}.cities_bronze\")\n","\n","    # Dicionário de correção para city_code\n","    city_code_mapping_dict = {\n","        \"?zmir\": \"Izmir\",\n","        \"Sanl?urfa\": \"Sanliurfa\",\n","        \"Diyarbak?r\": \"Diyarbakir\",\n","        \"Eski?ehir\": \"Eskisehir\",\n","        \"Adapazar?\": \"Adapazari\"\n","    }\n","\n","    # Construindo a lista de pares chave-valor para create_map\n","    city_code_map_args = []\n","    for k, v in city_code_mapping_dict.items():\n","        city_code_map_args.append(lit(k))\n","        city_code_map_args.append(lit(v))\n","    \n","    city_code_map_spark = create_map(*city_code_map_args)\n","\n","    df_cities_silver = df_cities_bronze.select(\n","        col(\"store_id\"),\n","        col(\"storetype_id\"), # Mantido em minúsculas\n","        col(\"store_size\"),\n","        # Retirada da coluna \"city_old_id\" por, aparentemente, se tratar de um id antigo\n","        # Aplicando o mapeamento para city_code e renomeando para city_name\n","        # coalesce garante que se o valor não estiver no mapa, o original (trimed) seja usado\n","        coalesce(city_code_map_spark.getItem(col(\"city_code\")), trim(col(\"city_code\"))).alias(\"city_name\"),\n","        # Foi retirada a coluna de country_id pois todos os dados são referentes ao mesmo país (Turquia)\n","    ).filter(\n","        col(\"store_id\").isNotNull() # Remover linhas onde store_id é nulo (chave primária)\n","    )\n","\n","    # Escrever a tabela Silver\n","    df_cities_silver.write \\\n","        .format(\"delta\") \\\n","        .mode(\"overwrite\") \\\n","        .saveAsTable(f\"{silver_lakehouse_name}.{silver_schema_name}.cities_silver\")\n","\n","    print(f\"Tabela 'cities_silver' criada com sucesso no Lakehouse '{silver_lakehouse_name}'.\")\n","    df_cities_silver.printSchema()\n","    df_cities_silver.show(5)\n","\n","except Exception as e:\n","    print(f\"ERRO: Falha ao processar 'cities_silver'. Detalhes do erro: {e}\")\n","\n","# --- 3. Processamento da Tabela 'product_bronze' -> 'product_silver' ---\n","print(\"\\n--- Processando product_bronze -> product_silver ---\")\n","try:\n","    df_product_bronze = spark.read.format(\"delta\").table(f\"{bronze_lakehouse_name}.{bronze_schema_name}.product_bronze\")\n","\n","    df_product_silver = df_product_bronze.select(\n","        col(\"product_id\"),\n","        # Tratamento de nulos em product_length, product_depth, product_width\n","        # Preenchendo nulos com 0.0 (double)\n","        coalesce(col(\"product_length\"), lit(0.0)).alias(\"product_length\"),\n","        coalesce(col(\"product_depth\"), lit(0.0)).alias(\"product_depth\"),\n","        coalesce(col(\"product_width\"), lit(0.0)).alias(\"product_width\"),\n","        # Tratamento de NULLs em cluster_id\n","        when(col(\"cluster_id\").isNull(), \"unknown_cluster\") # Mantido em minúsculas\n","        .otherwise(trim(col(\"cluster_id\")))\n","        .alias(\"cluster_id\"),\n","        trim(col(\"hierarchy1_id\")).alias(\"hierarchy1_id\"), # Mantido em minúsculas\n","        trim(col(\"hierarchy2_id\")).alias(\"hierarchy2_id\"), # Mantido em minúsculas\n","        trim(col(\"hierarchy3_id\")).alias(\"hierarchy3_id\"), # Mantido em minúsculas\n","        trim(col(\"hierarchy4_id\")).alias(\"hierarchy4_id\"), # Mantido em minúsculas\n","        trim(col(\"hierarchy5_id\")).alias(\"hierarchy5_id\")  # Mantido em minúsculas\n","    ).filter(\n","        col(\"product_id\").isNotNull() # Remover linhas onde product_id é nulo\n","    ).dropDuplicates([\"product_id\"]) # Remover duplicatas baseadas em product_id\n","\n","    # Escrever a tabela Silver\n","    df_product_silver.write \\\n","        .format(\"delta\") \\\n","        .mode(\"overwrite\") \\\n","        .saveAsTable(f\"{silver_lakehouse_name}.{silver_schema_name}.product_silver\")\n","\n","    print(f\"Tabela 'product_silver' criada com sucesso no Lakehouse '{silver_lakehouse_name}'.\")\n","    df_product_silver.printSchema()\n","    df_product_silver.show(5)\n","\n","except Exception as e:\n","    print(f\"ERRO: Falha ao processar 'product_silver'. Detalhes do erro: {e}\")\n","\n","\n","# --- 4. Processamento da Tabela 'sales_bronze' -> 'sales_silver' ---\n","print(\"\\n--- Processando sales_bronze -> sales_silver ---\")\n","try:\n","    df_sales_bronze = spark.read.format(\"delta\").table(f\"{bronze_lakehouse_name}.{bronze_schema_name}.sales_bronze\")\n","\n","    # PRÉ-PASSO CRÍTICO: Remover a coluna '_c0' ANTES do select principal\n","    if \"_c0\" in df_sales_bronze.columns:\n","        df_sales_bronze_cleaned = df_sales_bronze.drop(\"_c0\")\n","    else:\n","        df_sales_bronze_cleaned = df_sales_bronze # Se _c0 já não existe, não faz nada\n","    \n","    # --- CALCULAR A MEDIANA DE 'price' ANTES DAS TRANSFORMAÇÕES ---\n","    # É importante calcular a mediana a partir dos valores válidos (não nulos)\n","    # Cast para DoubleType antes de calcular a média para garantir que a agregação funcione corretamente\n","    median_price_value = df_sales_bronze_cleaned.select(approx_percentile(col(\"price\").cast(DoubleType()),0.5)).collect()[0][0]\n","\n","    # Tratar o caso em que a mediana pode ser None (se a coluna for toda nula)\n","    # Se a média for None, podemos definir um valor padrão, por exemplo, 0.0 ou levantar um erro\n","    if median_price_value is None:\n","        print(\"Aviso: A coluna 'price' contém apenas valores nulos. Usando 0.0 como média de fallback.\")\n","        median_price_value = 0.0 # Valor padrão se não houver valores para calcular a média\n","    \n","    print(f\"Mediana calculada para a coluna 'price': {median_price_value}\")\n","\n","    # Construindo as expressões de coluna de forma mais verbosa e explícita\n","    # Isso isola cada etapa e evita potenciais ambiguidades para o otimizador do Spark\n","\n","    # Numeric Columns\n","    sales_col_raw = col(\"sales\")\n","    sales_cast_double = sales_col_raw.cast(DoubleType())\n","    sales_final = when(sales_cast_double.isNotNull(), sales_cast_double).otherwise(lit(0.0)).alias(\"sales\")\n","\n","    revenue_col_raw = col(\"revenue\")\n","    revenue_cast_double = revenue_col_raw.cast(DoubleType())\n","    revenue_final = when(revenue_cast_double.isNotNull(), revenue_cast_double).otherwise(lit(0.0)).alias(\"revenue\")\n","\n","    stock_col_raw = col(\"stock\")\n","    stock_cast_double = stock_col_raw.cast(DoubleType())\n","    stock_final = when(stock_cast_double.isNotNull(), stock_cast_double).otherwise(lit(0.0)).alias(\"stock\")\n","\n"," # Numeric Column (Imputação com a MEDIANA para price)\n","    price_col_raw = col(\"price\")\n","    price_cast_double = price_col_raw.cast(DoubleType())\n","    # Usar o valor da média calculada\n","    price_final = when(price_cast_double.isNotNull(), price_cast_double).otherwise(lit(median_price_value)).alias(\"price\")\n","\n","    promo_discount_2_col_raw = col(\"promo_discount_2\")\n","    promo_discount_2_cast_double = promo_discount_2_col_raw.cast(DoubleType())\n","    promo_discount_2_final = when(promo_discount_2_cast_double.isNotNull(), promo_discount_2_cast_double).otherwise(lit(0.0)).alias(\"promo_discount_2\")\n","\n","    # String/Categorical Columns\n","    promo_type_1_raw = col(\"promo_type_1\")\n","    promo_type_1_trimmed_lower = trim(promo_type_1_raw)\n","    promo_type_1_final = when(promo_type_1_trimmed_lower == \"na\", \"no_promo\").otherwise(promo_type_1_trimmed_lower).alias(\"promo_type_1\")\n","\n","    promo_bin_1_raw = col(\"promo_bin_1\")\n","    promo_bin_1_trimmed_lower = trim(promo_bin_1_raw)\n","    promo_bin_1_final = when(promo_bin_1_trimmed_lower == \"na\", \"no_bin\").otherwise(promo_bin_1_trimmed_lower).alias(\"promo_bin_1\")\n","\n","    promo_type_2_raw = col(\"promo_type_2\")\n","    promo_type_2_trimmed_lower = trim(promo_type_2_raw)\n","    promo_type_2_final = when(promo_type_2_trimmed_lower == \"na\", \"no_promo\").otherwise(promo_type_2_trimmed_lower).alias(\"promo_type_2\")\n","\n","    promo_bin_2_raw = col(\"promo_bin_2\")\n","    promo_bin_2_trimmed_lower = trim(promo_bin_2_raw)\n","    promo_bin_2_final = when(promo_bin_2_trimmed_lower == \"na\", \"no_bin\").otherwise(promo_bin_2_trimmed_lower).alias(\"promo_bin_2\")\n","\n","    promo_discount_type_2_raw = col(\"promo_discount_type_2\")\n","    promo_discount_type_2_trimmed_lower = trim(promo_discount_type_2_raw)\n","    promo_discount_type_2_final = when(promo_discount_type_2_trimmed_lower == \"na\", \"no_discount_type\").otherwise(promo_discount_type_2_trimmed_lower).alias(\"promo_discount_type_2\")\n","\n","\n","    # Lista final de colunas para o select, usando as variáveis explicitamente definidas\n","    sales_silver_columns_definitions = [\n","        col(\"store_id\"),\n","        col(\"product_id\"),\n","        col(\"date\"), # 'date' já está como DateType\n","\n","        sales_final,\n","        revenue_final,\n","        stock_final,\n","        price_final,\n","        \n","        promo_type_1_final,\n","        promo_bin_1_final,\n","        promo_type_2_final,\n","        promo_bin_2_final,\n","        \n","        promo_discount_2_final,\n","        promo_discount_type_2_final\n","    ]\n","\n","    # Aplicar as transformações usando a lista de definições de colunas NO DATAFRAME LIMPO\n","    df_sales_silver = df_sales_bronze_cleaned.select(*sales_silver_columns_definitions).filter(\n","        # Remover linhas onde store_id, product_id ou date são nulos, pois são chaves essenciais\n","        col(\"store_id\").isNotNull() &\n","        col(\"product_id\").isNotNull() &\n","        col(\"date\").isNotNull()\n","    ) # Opcional: .dropDuplicates([\"store_id\", \"product_id\", \"date\"]) se a combinação for única por dia\n","\n","    # Escrever a tabela Silver\n","    df_sales_silver.write \\\n","        .format(\"delta\") \\\n","        .mode(\"overwrite\") \\\n","        .saveAsTable(f\"{silver_lakehouse_name}.{silver_schema_name}.sales_silver\")\n","\n","    print(f\"Tabela 'sales_silver' criada com sucesso no Lakehouse '{silver_lakehouse_name}'.\")\n","    df_sales_silver.printSchema()\n","    df_sales_silver.show(5)\n","\n","except Exception as e:\n","    print(f\"ERRO: Falha ao processar 'sales_silver'. Detalhes do erro: {e}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"--- Processamento da Camada Bronze para Silver Concluído! ---\")\n","print(\"As tabelas Delta foram criadas no Lakehouse 'Projeto_II_Silver_', Schema 'Projeto_II_Silver_'.\")\n","print(\"=\"*80)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"d09098d8-0fb4-4392-b623-961b6bdaa95e","normalized_state":"finished","queued_time":"2025-06-28T11:15:58.896451Z","session_start_time":"2025-06-28T11:15:58.8978484Z","execution_start_time":"2025-06-28T11:16:10.2872452Z","execution_finish_time":"2025-06-28T11:17:07.7108265Z","parent_msg_id":"e6604bbe-0966-418d-a7f7-c2c35056b6ab"},"text/plain":"StatementMeta(, d09098d8-0fb4-4392-b623-961b6bdaa95e, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processamento da Camada Bronze -> Silver iniciado.\nLendo do Lakehouse Bronze: 'Projeto_II_Bronze_', Schema: 'Projeto_II_Bronze_'\nEscrevendo no Lakehouse Silver: 'Projeto_II_Silver_', Schema: 'Projeto_II_Silver_'\n--------------------------------------------------------------------------------\n\n--- Processando cities_bronze -> cities_silver ---\nTabela 'cities_silver' criada com sucesso no Lakehouse 'Projeto_II_Silver_'.\nroot\n |-- store_id: string (nullable = true)\n |-- storetype_id: string (nullable = true)\n |-- store_size: integer (nullable = true)\n |-- city_name: string (nullable = true)\n\n+--------+------------+----------+---------+\n|store_id|storetype_id|store_size|city_name|\n+--------+------------+----------+---------+\n|   S0036|        ST04|        21|  Denizli|\n|   S0005|        ST04|        19|  Denizli|\n|   S0104|        ST04|        47|   Ankara|\n|   S0068|        ST03|        14|    Izmir|\n|   S0086|        ST03|        12|    Izmir|\n+--------+------------+----------+---------+\nonly showing top 5 rows\n\n\n--- Processando product_bronze -> product_silver ---\nTabela 'product_silver' criada com sucesso no Lakehouse 'Projeto_II_Silver_'.\nroot\n |-- product_id: string (nullable = true)\n |-- product_length: double (nullable = false)\n |-- product_depth: double (nullable = false)\n |-- product_width: double (nullable = false)\n |-- cluster_id: string (nullable = true)\n |-- hierarchy1_id: string (nullable = true)\n |-- hierarchy2_id: string (nullable = true)\n |-- hierarchy3_id: string (nullable = true)\n |-- hierarchy4_id: string (nullable = true)\n |-- hierarchy5_id: string (nullable = true)\n\n+----------+--------------+-------------+-------------+---------------+-------------+-------------+-------------+-------------+-------------+\n|product_id|product_length|product_depth|product_width|     cluster_id|hierarchy1_id|hierarchy2_id|hierarchy3_id|hierarchy4_id|hierarchy5_id|\n+----------+--------------+-------------+-------------+---------------+-------------+-------------+-------------+-------------+-------------+\n|     P0000|           5.0|         20.0|         12.0|unknown_cluster|          H00|        H0004|      H000401|    H00040105|  H0004010534|\n|     P0001|          13.5|         22.0|         20.0|      cluster_5|          H01|        H0105|      H010501|    H01050100|  H0105010006|\n|     P0002|          22.0|         40.0|         22.0|      cluster_0|          H03|        H0315|      H031508|    H03150800|  H0315080028|\n|     P0004|           2.0|         13.0|          4.0|      cluster_3|          H03|        H0314|      H031405|    H03140500|  H0314050003|\n|     P0005|          16.0|         30.0|         16.0|      cluster_9|          H03|        H0312|      H031211|    H03121109|  H0312110917|\n+----------+--------------+-------------+-------------+---------------+-------------+-------------+-------------+-------------+-------------+\nonly showing top 5 rows\n\n\n--- Processando sales_bronze -> sales_silver ---\nMediana calculada para a coluna 'price': 8.0\nTabela 'sales_silver' criada com sucesso no Lakehouse 'Projeto_II_Silver_'.\nroot\n |-- store_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- date: date (nullable = true)\n |-- sales: double (nullable = true)\n |-- revenue: double (nullable = true)\n |-- stock: double (nullable = true)\n |-- price: double (nullable = true)\n |-- promo_type_1: string (nullable = true)\n |-- promo_bin_1: string (nullable = true)\n |-- promo_type_2: string (nullable = true)\n |-- promo_bin_2: string (nullable = true)\n |-- promo_discount_2: double (nullable = true)\n |-- promo_discount_type_2: string (nullable = true)\n\n+--------+----------+----------+-----+-------+-----+-----+------------+-----------+------------+-----------+----------------+---------------------+\n|store_id|product_id|      date|sales|revenue|stock|price|promo_type_1|promo_bin_1|promo_type_2|promo_bin_2|promo_discount_2|promo_discount_type_2|\n+--------+----------+----------+-----+-------+-----+-----+------------+-----------+------------+-----------+----------------+---------------------+\n|   S0002|     P0001|2017-01-02|  0.0|    0.0|  8.0| 6.25|        PR14|         NA|        PR03|         NA|             0.0|                   NA|\n|   S0002|     P0005|2017-01-02|  0.0|    0.0| 11.0| 33.9|        PR14|         NA|        PR03|         NA|             0.0|                   NA|\n|   S0002|     P0011|2017-01-02|  0.0|    0.0|  9.0| 49.9|        PR14|         NA|        PR03|         NA|             0.0|                   NA|\n|   S0002|     P0015|2017-01-02|  1.0|   2.41| 19.0|  2.6|        PR14|         NA|        PR03|         NA|             0.0|                   NA|\n|   S0002|     P0017|2017-01-02|  0.0|    0.0| 12.0| 1.49|        PR14|         NA|        PR03|         NA|             0.0|                   NA|\n+--------+----------+----------+-----+-------+-----+-----+------------+-----------+------------+-----------+----------------+---------------------+\nonly showing top 5 rows\n\n\n================================================================================\n--- Processamento da Camada Bronze para Silver Concluído! ---\nAs tabelas Delta foram criadas no Lakehouse 'Projeto_II_Silver_', Schema 'Projeto_II_Silver_'.\n================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"064d3e27-03c4-4c22-8813-99657bf4f812"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"70bc6dd1-ec94-47fe-96fa-70959ab0f42c"}],"default_lakehouse":"70bc6dd1-ec94-47fe-96fa-70959ab0f42c","default_lakehouse_name":"Projeto_II_Silver_","default_lakehouse_workspace_id":"206184e5-f2b5-4d4e-b443-0c5761a9b558"}}},"nbformat":4,"nbformat_minor":5}